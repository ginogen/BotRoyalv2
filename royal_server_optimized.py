#!/usr/bin/env python3
"""
üöÄ ROYAL BOT SERVER - MAXIMUM EFFICIENCY EDITION
Integrates all optimization systems for Railway deployment:
- Hybrid Context Management (PostgreSQL + Redis + Memory)
- Advanced Message Queue with priorities
- Dynamic Worker Pool with auto-scaling
- Circuit breaker patterns
- Smart rate limiting
- Real-time monitoring
"""

import os
import json
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import time
import uuid

# FastAPI and middleware
from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# Our optimization systems
from hybrid_context_manager import hybrid_context_manager, initialize_hybrid_context
from advanced_message_queue import (
    advanced_queue, initialize_queue, MessageData, 
    MessageSource, MessagePriority
)
from dynamic_worker_pool import initialize_worker_pool, shutdown_worker_pool, dynamic_pool

# Royal agents import
try:
    from royal_agents import run_contextual_conversation_sync
except ImportError:
    # Fallback if royal_agents not available
    def run_contextual_conversation_sync(user_id: str, message: str) -> str:
        return "Sistema en modo de prueba. Royal agents no disponible."

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =====================================================
# CONFIGURATION
# =====================================================

# Environment variables with Railway defaults
PORT = int(os.getenv("PORT", 8000))
ENVIRONMENT = os.getenv("RAILWAY_ENVIRONMENT", "development")

# External service configurations
CHATWOOT_API_URL = os.getenv("CHATWOOT_API_URL")
CHATWOOT_API_TOKEN = os.getenv("CHATWOOT_API_TOKEN") 
CHATWOOT_ACCOUNT_ID = os.getenv("CHATWOOT_ACCOUNT_ID")
EVOLUTION_API_URL = os.getenv("EVOLUTION_API_URL")
EVOLUTION_API_TOKEN = os.getenv("EVOLUTION_API_TOKEN")
INSTANCE_NAME = os.getenv("INSTANCE_NAME", "F1_Retencion")

# Performance tuning
ENABLE_PERFORMANCE_MONITORING = os.getenv("ENABLE_PERFORMANCE_MONITORING", "true").lower() == "true"
ENABLE_REQUEST_LOGGING = os.getenv("ENABLE_REQUEST_LOGGING", "true").lower() == "true"

# =====================================================
# FASTAPI APP SETUP
# =====================================================

app = FastAPI(
    title="Royal Bot - Maximum Efficiency Edition",
    description="Optimized Royal Bot with advanced queueing, context management, and auto-scaling",
    version="3.0.0",
    docs_url="/admin/docs" if ENVIRONMENT == "development" else None,
    redoc_url="/admin/redoc" if ENVIRONMENT == "development" else None
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global state
startup_time = datetime.now()
request_count = 0
system_metrics = {
    'requests_processed': 0,
    'messages_queued': 0,
    'average_response_time': 0.0,
    'last_health_check': None
}

# =====================================================
# PYDANTIC MODELS
# =====================================================

class TestMessage(BaseModel):
    message: str
    user_id: Optional[str] = None
    priority: Optional[str] = "normal"

class WebhookMessage(BaseModel):
    message: str
    user_id: str
    source: str
    conversation_id: Optional[str] = None
    phone: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = {}

# =====================================================
# CORE MESSAGE PROCESSOR
# =====================================================

async def process_royal_message(user_id: str, message: str, message_data: Optional['MessageData'] = None) -> str:
    """
    Core message processor that integrates with all our systems.
    This is what gets called by the worker pool.
    """
    start_time = time.time()
    
    try:
        # Log the processing start
        logger.info(f"ü§ñ Processing message for {user_id}: {message[:50]}...")
        
        # Process with Royal agent (using sync version for thread compatibility)
        response = run_contextual_conversation_sync(user_id, message)
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Update global metrics
        system_metrics['average_response_time'] = (
            system_metrics['average_response_time'] * 0.9 + processing_time * 0.1
        )
        
        # Send response back to appropriate channel
        if message_data:
            await send_response_to_channel(user_id, response, message_data)
        
        logger.info(f"‚úÖ Message processed for {user_id} in {processing_time:.2f}s")
        return response
        
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"‚ùå Processing failed for {user_id} after {processing_time:.2f}s: {e}")
        
        # Return friendly error message
        error_response = (
            "Disculp√°, tuve un problemita t√©cnico. "
            "Dame un momento que ya te ayudo. "
            "Si es urgente, pod√©s contactar directamente a nuestros locales."
        )
        
        # Try to send error response
        if message_data:
            try:
                await send_response_to_channel(user_id, error_response, message_data)
            except:
                pass  # Don't fail if we can't send error response
        
        return error_response

async def send_response_to_channel(user_id: str, response: str, message_data: 'MessageData'):
    """Send response back to the appropriate channel (WhatsApp, Chatwoot, etc.)"""
    
    try:
        if message_data.source == MessageSource.EVOLUTION and message_data.phone:
            # Send to WhatsApp via Evolution API
            success = await send_evolution_message(message_data.phone, response)
            if success:
                logger.info(f"üì± Response sent to WhatsApp: {user_id}")
            else:
                logger.error(f"‚ùå Failed to send WhatsApp response: {user_id}")
                
        elif message_data.source == MessageSource.CHATWOOT and message_data.conversation_id:
            # Send to Chatwoot
            success = await send_chatwoot_message(message_data.conversation_id, response)
            if success:
                logger.info(f"üí¨ Response sent to Chatwoot: {user_id}")
            else:
                logger.error(f"‚ùå Failed to send Chatwoot response: {user_id}")
                
        else:
            logger.warning(f"‚ö†Ô∏è No channel configured for response: {user_id} (source: {message_data.source})")
            
    except Exception as e:
        logger.error(f"‚ùå Error sending response to channel: {e}")

# =====================================================
# EXTERNAL SERVICE INTEGRATIONS
# =====================================================

async def send_chatwoot_message(conversation_id: str, message: str) -> bool:
    """Send message to Chatwoot"""
    if not all([CHATWOOT_API_URL, CHATWOOT_API_TOKEN, CHATWOOT_ACCOUNT_ID]):
        logger.warning("‚ö†Ô∏è Chatwoot not configured")
        return False
    
    try:
        import httpx
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            headers = {
                "api_access_token": CHATWOOT_API_TOKEN,
                "Content-Type": "application/json"
            }
            
            payload = {
                "content": message,
                "message_type": "outgoing"
            }
            
            response = await client.post(
                f"{CHATWOOT_API_URL}/api/v1/accounts/{CHATWOOT_ACCOUNT_ID}/conversations/{conversation_id}/messages",
                headers=headers,
                json=payload
            )
            
            success = response.status_code in [200, 201]
            if success:
                logger.info(f"‚úÖ Chatwoot message sent to conversation {conversation_id}")
            else:
                logger.error(f"‚ùå Chatwoot send failed: {response.status_code}")
            
            return success
            
    except Exception as e:
        logger.error(f"‚ùå Chatwoot send error: {e}")
        return False

async def send_evolution_message(phone: str, message: str) -> bool:
    """Send message via Evolution API"""
    if not all([EVOLUTION_API_URL, EVOLUTION_API_TOKEN]):
        logger.warning("‚ö†Ô∏è Evolution API not configured")
        return False
    
    try:
        import httpx
        
        # Use phone number exactly as received (Evolution API expects no + prefix)
        formatted_phone = phone
        
        # Log the request details for debugging
        logger.info(f"üîÑ Sending message to Evolution API:")
        logger.info(f"   URL: {EVOLUTION_API_URL}/message/sendText/{INSTANCE_NAME}")
        logger.info(f"   Phone: {formatted_phone}")
        logger.info(f"   Message length: {len(message)}")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            headers = {
                "apikey": EVOLUTION_API_TOKEN,
                "Content-Type": "application/json"
            }
            
            payload = {
                "number": formatted_phone,
                "text": message
            }
            
            response = await client.post(
                f"{EVOLUTION_API_URL}/message/sendText/{INSTANCE_NAME}",
                headers=headers,
                json=payload
            )
            
            # Check for success (200 or 201 are both success)
            if response.status_code in [200, 201]:
                logger.info(f"‚úÖ Evolution message sent to {formatted_phone}")
                return True
            
            # Enhanced error logging for actual errors
            logger.error(f"‚ùå Evolution API Error:")
            logger.error(f"   Status Code: {response.status_code}")
            logger.error(f"   Response: {response.text}")
            logger.error(f"   Headers: {dict(response.headers)}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Evolution send error: {e}")
        return False

# =====================================================
# MESSAGE PROCESSING PIPELINE
# =====================================================

async def process_message_pipeline(
    user_id: str,
    message: str,
    source: MessageSource,
    conversation_id: Optional[str] = None,
    phone: Optional[str] = None,
    priority: MessagePriority = MessagePriority.NORMAL,
    metadata: Optional[Dict] = None
) -> bool:
    """
    Complete message processing pipeline:
    1. Create message data
    2. Add to queue
    3. Worker pool processes it
    4. Response is sent back via appropriate channel
    """
    
    # Create message data
    message_data = MessageData(
        user_id=user_id,
        message=message,
        source=source,
        priority=priority,
        conversation_id=conversation_id,
        phone=phone,
        metadata=metadata or {}
    )
    
    # Add to advanced queue
    success = await advanced_queue.add_message(message_data)
    
    if success:
        system_metrics['messages_queued'] += 1
        logger.info(f"üì• Message queued: {user_id} via {source.value}")
    else:
        logger.warning(f"‚ö†Ô∏è Message not queued (duplicate?): {user_id}")
    
    return success

# =====================================================
# BACKGROUND TASKS
# =====================================================

async def response_sender_task():
    """Background task that monitors completed messages and sends responses"""
    logger.info("üì° Response sender task started")
    
    # This would typically involve polling completed messages
    # and sending responses via the appropriate channels
    # For now, we'll implement a simple monitoring loop
    
    while True:
        try:
            # Get queue stats for monitoring
            if advanced_queue and advanced_queue.pg_pool:
                # Here we could check for completed messages that need response sending
                # This is where the integration with Chatwoot/Evolution would happen
                pass
            
            await asyncio.sleep(5)  # Check every 5 seconds
            
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"‚ùå Response sender error: {e}")
            await asyncio.sleep(30)
    
    logger.info("üì° Response sender task stopped")

async def metrics_collector_task():
    """Background task for collecting and storing metrics"""
    logger.info("üìä Metrics collector started")
    
    while True:
        try:
            if ENABLE_PERFORMANCE_MONITORING:
                # Collect metrics from all systems
                current_time = datetime.now()
                
                # Update system health check time
                system_metrics['last_health_check'] = current_time
                
                # Here we could store metrics in database for long-term analysis
                # For now, we just update in-memory metrics
                
            await asyncio.sleep(60)  # Collect every minute
            
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"‚ùå Metrics collector error: {e}")
            await asyncio.sleep(120)
    
    logger.info("üìä Metrics collector stopped")

# =====================================================
# API ENDPOINTS
# =====================================================

@app.get("/")
async def root():
    """Service status and information"""
    uptime = datetime.now() - startup_time
    
    return {
        "service": "Royal Bot - Maximum Efficiency Edition",
        "version": "3.0.0",
        "status": "active",
        "uptime_seconds": int(uptime.total_seconds()),
        "uptime_human": str(uptime),
        "environment": ENVIRONMENT,
        "features": [
            "‚úÖ Hybrid Context Management (PostgreSQL + Redis + Memory)",
            "‚úÖ Advanced Priority Message Queue",
            "‚úÖ Dynamic Auto-scaling Worker Pool",
            "‚úÖ Circuit Breaker Protection",
            "‚úÖ Real-time Performance Monitoring",
            "‚úÖ Smart Rate Limiting",
            "‚úÖ Chatwoot Integration",
            "‚úÖ Evolution API (WhatsApp) Integration"
        ],
        "metrics": {
            "requests_processed": system_metrics['requests_processed'],
            "messages_queued": system_metrics['messages_queued'],
            "average_response_time": round(system_metrics['average_response_time'], 3)
        }
    }

@app.get("/health")
async def health_check():
    """Comprehensive health check endpoint"""
    start_time = time.time()
    
    health = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "checks": {},
        "overall_status": "healthy"
    }
    
    # Check hybrid context manager
    try:
        context_health = await hybrid_context_manager.health_check()
        health["checks"]["context_manager"] = context_health
        if context_health["status"] != "healthy":
            health["overall_status"] = "degraded"
    except Exception as e:
        health["checks"]["context_manager"] = {"status": "unhealthy", "error": str(e)}
        health["overall_status"] = "degraded"
    
    # Check message queue
    try:
        queue_health = await advanced_queue.health_check()
        health["checks"]["message_queue"] = queue_health
        if queue_health["status"] != "healthy":
            health["overall_status"] = "degraded"
    except Exception as e:
        health["checks"]["message_queue"] = {"status": "unhealthy", "error": str(e)}
        health["overall_status"] = "degraded"
    
    # Check worker pool
    try:
        if dynamic_pool:
            pool_health = await dynamic_pool.health_check()
            health["checks"]["worker_pool"] = pool_health
            if pool_health["status"] != "healthy":
                health["overall_status"] = "degraded"
        else:
            health["checks"]["worker_pool"] = {"status": "not_initialized"}
    except Exception as e:
        health["checks"]["worker_pool"] = {"status": "unhealthy", "error": str(e)}
        health["overall_status"] = "degraded"
    
    # Response time check
    health_check_time = (time.time() - start_time) * 1000
    health["response_time_ms"] = round(health_check_time, 2)
    
    # Overall status logic
    unhealthy_checks = sum(1 for check in health["checks"].values() 
                          if check.get("status") == "unhealthy")
    
    if unhealthy_checks > 0:
        health["overall_status"] = "unhealthy"
    elif health["overall_status"] != "healthy":
        health["overall_status"] = "degraded"
    
    # Set HTTP status code based on health
    status_code = 200
    if health["overall_status"] == "degraded":
        status_code = 200  # Still operational
    elif health["overall_status"] == "unhealthy":
        status_code = 503  # Service unavailable
    
    return JSONResponse(content=health, status_code=status_code)

@app.post("/webhook/chatwoot")
async def chatwoot_webhook(request: Request, background_tasks: BackgroundTasks):
    """Chatwoot webhook handler with advanced processing"""
    global request_count
    request_count += 1
    system_metrics['requests_processed'] += 1
    
    try:
        data = await request.json()
        
        # Log webhook if enabled
        if ENABLE_REQUEST_LOGGING:
            logger.info(f"üì® Chatwoot webhook: {json.dumps(data, indent=2)[:500]}...")
        
        # Process only incoming messages from contacts
        if (data.get("event") == "message_created" and 
            data.get("message_type") == "incoming" and
            data.get("sender", {}).get("type") == "contact"):
            
            conversation = data.get("conversation", {})
            content = data.get("content", "").strip()
            
            if content and conversation.get("id"):
                contact_id = data.get("sender", {}).get("id")
                
                if contact_id is not None:
                    user_id = f"chatwoot_{str(contact_id)}"
                    conversation_id = str(conversation["id"])
                    
                    # Determine priority based on content
                    priority = MessagePriority.NORMAL
                    if any(word in content.lower() for word in ['urgente', 'problema', 'error']):
                        priority = MessagePriority.HIGH
                    
                    # Add to processing pipeline
                    success = await process_message_pipeline(
                        user_id=user_id,
                        message=content,
                        source=MessageSource.CHATWOOT,
                        conversation_id=conversation_id,
                        priority=priority,
                        metadata={
                            "contact_id": contact_id,
                            "conversation_type": conversation.get("channel"),
                            "webhook_data": data
                        }
                    )
                    
                    if success:
                        # Schedule response sending (this would be handled by worker completion)
                        # background_tasks.add_task(send_chatwoot_response, conversation_id, user_id)
                        pass
        
        return {"status": "received", "processed": True}
        
    except Exception as e:
        logger.error(f"‚ùå Chatwoot webhook error: {e}")
        raise HTTPException(status_code=500, detail="Webhook processing failed")

@app.post("/webhook/evolution")
async def evolution_webhook(request: Request, background_tasks: BackgroundTasks):
    """Evolution API webhook handler with advanced processing"""
    global request_count
    request_count += 1
    system_metrics['requests_processed'] += 1
    
    try:
        data = await request.json()
        
        # Log webhook if enabled
        if ENABLE_REQUEST_LOGGING:
            logger.info(f"üì± Evolution webhook: {json.dumps(data, indent=2)[:500]}...")
        
        # Only process incoming messages from users (not our own messages)
        event = data.get("event", "")
        message_data_raw = data.get("data", {})
        
        # Filter out events we don't want to process
        if event not in ["messages.upsert"]:
            logger.info(f"üîá Ignoring event: {event}")
            return {"status": "received", "ignored": event}
        
        # Check if message is from us (fromMe: true)
        from_me = message_data_raw.get("key", {}).get("fromMe", False)
        if from_me:
            logger.info("üîá Ignoring message from bot itself (fromMe: true)")
            return {"status": "received", "ignored": "fromMe"}
        
        # Extract message data
        message_content = message_data_raw.get("message", {}).get("conversation", "").strip()
        from_number = message_data_raw.get("key", {}).get("remoteJid", "").replace("@s.whatsapp.net", "")
        
        if message_content and from_number:
            user_id = f"whatsapp_{from_number}"
            
            # Auto-prioritize based on content
            priority = MessagePriority.NORMAL
            if any(word in message_content.lower() for word in ['urgente', 'problema', 'error']):
                priority = MessagePriority.HIGH
            elif any(word in message_content.lower() for word in ['comprar', 'precio', 'stock']):
                priority = MessagePriority.HIGH
            
            # Add to processing pipeline
            success = await process_message_pipeline(
                user_id=user_id,
                message=message_content,
                source=MessageSource.EVOLUTION,
                phone=from_number,
                priority=priority,
                metadata={
                    "instance": INSTANCE_NAME,
                    "webhook_data": data
                }
            )
        
        return {"status": "received", "processed": True}
        
    except Exception as e:
        logger.error(f"‚ùå Evolution webhook error: {e}")
        raise HTTPException(status_code=500, detail="Webhook processing failed")

@app.post("/test/message")
async def test_message(test_msg: TestMessage):
    """Test message endpoint with full processing pipeline"""
    user_id = test_msg.user_id or f"test_{uuid.uuid4().hex[:8]}"
    
    # Parse priority
    priority_map = {
        'urgent': MessagePriority.URGENT,
        'high': MessagePriority.HIGH,
        'normal': MessagePriority.NORMAL,
        'low': MessagePriority.LOW
    }
    priority = priority_map.get(test_msg.priority, MessagePriority.NORMAL)
    
    # Process through pipeline
    success = await process_message_pipeline(
        user_id=user_id,
        message=test_msg.message,
        source=MessageSource.TEST,
        priority=priority,
        metadata={"test_mode": True}
    )
    
    if not success:
        raise HTTPException(status_code=500, detail="Failed to queue message")
    
    # For test endpoint, we'll wait a bit and return some stats
    await asyncio.sleep(0.5)
    
    return {
        "status": "queued",
        "user_id": user_id,
        "priority": priority.name,
        "message": test_msg.message,
        "queue_info": "Message added to advanced priority queue for processing"
    }

# =====================================================
# MONITORING ENDPOINTS
# =====================================================

@app.get("/metrics")
async def get_metrics():
    """Get comprehensive system metrics"""
    metrics = {
        "system": system_metrics.copy(),
        "uptime": (datetime.now() - startup_time).total_seconds()
    }
    
    # Get context manager metrics
    try:
        context_metrics = await hybrid_context_manager.get_metrics()
        metrics["context_manager"] = context_metrics
    except Exception as e:
        metrics["context_manager"] = {"error": str(e)}
    
    # Get queue metrics
    try:
        queue_metrics = await advanced_queue.get_metrics()
        metrics["message_queue"] = queue_metrics
    except Exception as e:
        metrics["message_queue"] = {"error": str(e)}
    
    # Get worker pool metrics
    try:
        if dynamic_pool:
            pool_metrics = await dynamic_pool.get_metrics()
            metrics["worker_pool"] = pool_metrics
        else:
            metrics["worker_pool"] = {"status": "not_initialized"}
    except Exception as e:
        metrics["worker_pool"] = {"error": str(e)}
    
    return metrics

@app.get("/admin/queue")
async def admin_queue_stats():
    """Admin endpoint for queue statistics"""
    try:
        stats = await advanced_queue.get_queue_stats()
        return {
            "queue_stats": {
                "total_pending": stats.total_pending,
                "total_processing": stats.total_processing,
                "total_completed": stats.total_completed,
                "total_failed": stats.total_failed,
                "total_dead_letter": stats.total_dead_letter,
                "queue_depth": stats.queue_depth,
                "avg_processing_time": stats.avg_processing_time,
                "error_rate": stats.error_rate,
                "oldest_pending": stats.oldest_pending.isoformat() if stats.oldest_pending else None
            },
            "priority_breakdown": stats.pending_by_priority
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get queue stats: {e}")

@app.get("/admin/workers")
async def admin_worker_stats():
    """Admin endpoint for worker pool statistics"""
    try:
        if not dynamic_pool:
            return {"error": "Worker pool not initialized"}
        
        return await dynamic_pool.get_metrics()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get worker stats: {e}")

# =====================================================
# ADMIN ENDPOINTS FOR DYNAMIC CONTENT
# =====================================================

@app.post("/admin/add-training-example")
async def add_training_example(request: Request):
    """Add new training example without redeploying"""
    try:
        data = await request.json()
        
        # Validate required fields
        required_fields = ["tipo", "usuario", "bot", "descripcion"]
        if not all(field in data for field in required_fields):
            raise HTTPException(status_code=400, detail="Missing required fields")
        
        # Append to training file  
        training_file = "/app/Entrenamiento/Entrenamiento-Productos.txt"  # Railway path
        
        new_example = f"""
CONVERSACI√ìN EJEMPLO NUEVA:
Usuario: {data['usuario']}
Royal: {data['bot']}
Descripci√≥n: {data['descripcion']}
Agregado: {datetime.now().isoformat()}
"""
        
        with open(training_file, 'a', encoding='utf-8') as f:
            f.write(new_example)
        
        logger.info(f"‚úÖ New training example added: {data['tipo']}")
        
        return {
            "status": "success",
            "message": "Training example added successfully",
            "example": new_example
        }
        
    except Exception as e:
        logger.error(f"‚ùå Failed to add training example: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/add-business-info")
async def add_business_info(request: Request):
    """Add new business information dynamically - PERSISTENT in PostgreSQL"""
    try:
        data = await request.json()
        
        # Generate unique ID
        info_id = str(uuid.uuid4())[:8]
        
        # Prepare data for database
        new_info = {
            "id": info_id,
            "category": data.get("category", "general"),
            "title": data.get("title", ""),
            "content": data.get("content", ""),
            "keywords": data.get("keywords", []),
            "alternative_response": data.get("alternative_response", ""),
            "added": datetime.now().isoformat()
        }
        
        # Store in PostgreSQL for persistence
        if advanced_queue.pg_pool:
            conn = advanced_queue.pg_pool.getconn()
            try:
                cursor = conn.cursor()
                
                # Create table if not exists
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS bot_knowledge (
                        id VARCHAR(10) PRIMARY KEY,
                        category VARCHAR(50),
                        title TEXT,
                        content TEXT,
                        keywords TEXT,
                        alternative_response TEXT,
                        added TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        active BOOLEAN DEFAULT true
                    )
                """)
                
                # Insert new knowledge
                cursor.execute("""
                    INSERT INTO bot_knowledge 
                    (id, category, title, content, keywords, alternative_response, added)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                """, (
                    info_id,
                    new_info["category"],
                    new_info["title"], 
                    new_info["content"],
                    ','.join(new_info["keywords"]) if isinstance(new_info["keywords"], list) else str(new_info["keywords"]),
                    new_info["alternative_response"],
                    datetime.now()
                ))
                
                conn.commit()
                logger.info(f"‚úÖ Business info stored in DB: {new_info['title']}")
                
            finally:
                advanced_queue.pg_pool.putconn(conn)
        
        return {
            "status": "success",
            "message": "Information saved permanently in database",
            "info": new_info
        }
        
    except Exception as e:
        logger.error(f"‚ùå Failed to add business info: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/reload-training")
async def reload_training():
    """Force reload training data without restart"""
    try:
        # This would trigger the training parser to reload
        # For now, return success message
        logger.info("üîÑ Training data reload requested")
        
        return {
            "status": "success", 
            "message": "Training data will be reloaded on next request",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/business-info")
async def get_business_info():
    """Get current dynamic business information from PostgreSQL"""
    try:
        if not advanced_queue.pg_pool:
            return {"updated": "never", "info": [], "count": 0, "source": "no_db"}
        
        conn = advanced_queue.pg_pool.getconn()
        try:
            cursor = conn.cursor()
            
            # Get all active knowledge
            cursor.execute("""
                SELECT id, category, title, content, keywords, alternative_response, added
                FROM bot_knowledge 
                WHERE active = true 
                ORDER BY added DESC
            """)
            
            rows = cursor.fetchall()
            info_list = []
            
            for row in rows:
                info_item = {
                    "id": row[0],
                    "category": row[1],
                    "title": row[2],
                    "content": row[3],
                    "keywords": row[4].split(',') if row[4] else [],
                    "alternative_response": row[5] or "",
                    "added": row[6].isoformat() if row[6] else ""
                }
                info_list.append(info_item)
            
            # Get latest update time
            cursor.execute("SELECT MAX(added) FROM bot_knowledge WHERE active = true")
            latest_update = cursor.fetchone()[0]
            
            return {
                "updated": latest_update.isoformat() if latest_update else "never",
                "info": info_list,
                "count": len(info_list),
                "source": "postgresql"
            }
            
        finally:
            advanced_queue.pg_pool.putconn(conn)
        
    except Exception as e:
        logger.error(f"‚ùå Failed to get business info: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin", response_class=HTMLResponse)
async def admin_panel():
    """Serve admin panel HTML"""
    try:
        with open("/app/admin_panel.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        # Fallback for development
        try:
            with open("admin_panel.html", "r", encoding="utf-8") as f:
                return HTMLResponse(content=f.read())
        except FileNotFoundError:
            return HTMLResponse(content="<h1>Admin panel not found</h1>", status_code=404)

@app.delete("/admin/delete-info/{info_id}")
async def delete_business_info(info_id: str):
    """Delete/deactivate business information"""
    try:
        if not advanced_queue.pg_pool:
            raise HTTPException(status_code=500, detail="Database not available")
        
        conn = advanced_queue.pg_pool.getconn()
        try:
            cursor = conn.cursor()
            
            # Soft delete - mark as inactive
            cursor.execute("""
                UPDATE bot_knowledge 
                SET active = false 
                WHERE id = %s
            """, (info_id,))
            
            if cursor.rowcount > 0:
                conn.commit()
                logger.info(f"‚úÖ Knowledge deleted: {info_id}")
                return {"status": "success", "message": "Information deleted"}
            else:
                raise HTTPException(status_code=404, detail="Information not found")
                
        finally:
            advanced_queue.pg_pool.putconn(conn)
            
    except Exception as e:
        logger.error(f"‚ùå Failed to delete info: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/search-info")
async def search_business_info(q: str = "", category: str = ""):
    """Search business information"""
    try:
        if not advanced_queue.pg_pool:
            return {"results": [], "count": 0}
        
        conn = advanced_queue.pg_pool.getconn()
        try:
            cursor = conn.cursor()
            
            where_conditions = ["active = true"]
            params = []
            
            if q:
                where_conditions.append("(title ILIKE %s OR content ILIKE %s OR keywords ILIKE %s)")
                params.extend([f"%{q}%", f"%{q}%", f"%{q}%"])
            
            if category:
                where_conditions.append("category = %s")
                params.append(category)
            
            query = f"""
                SELECT id, category, title, content, keywords, alternative_response, added
                FROM bot_knowledge 
                WHERE {' AND '.join(where_conditions)}
                ORDER BY added DESC
                LIMIT 50
            """
            
            cursor.execute(query, params)
            rows = cursor.fetchall()
            
            results = []
            for row in rows:
                results.append({
                    "id": row[0],
                    "category": row[1],
                    "title": row[2],
                    "content": row[3][:200] + "..." if len(row[3]) > 200 else row[3],
                    "keywords": row[4].split(',') if row[4] else [],
                    "added": row[6].isoformat() if row[6] else ""
                })
            
            return {"results": results, "count": len(results)}
            
        finally:
            advanced_queue.pg_pool.putconn(conn)
            
    except Exception as e:
        logger.error(f"‚ùå Search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =====================================================
# STARTUP AND SHUTDOWN HANDLERS
# =====================================================

@app.on_event("startup")
async def startup_event():
    """Initialize all systems on startup"""
    logger.info("üöÄ Starting Royal Bot - Maximum Efficiency Edition")
    
    # Initialize hybrid context manager
    logger.info("üß† Initializing hybrid context manager...")
    await initialize_hybrid_context()
    
    # Initialize advanced message queue
    logger.info("üì¨ Initializing advanced message queue...")
    await initialize_queue()
    
    # Initialize dynamic worker pool with our message processor
    logger.info("‚ö° Initializing dynamic worker pool...")
    await initialize_worker_pool(process_royal_message)
    
    # Start background tasks
    logger.info("üì° Starting background tasks...")
    asyncio.create_task(response_sender_task())
    asyncio.create_task(metrics_collector_task())
    
    logger.info("‚úÖ Royal Bot - Maximum Efficiency Edition started successfully!")
    logger.info(f"   Environment: {ENVIRONMENT}")
    logger.info(f"   Port: {PORT}")
    logger.info(f"   Monitoring: {'Enabled' if ENABLE_PERFORMANCE_MONITORING else 'Disabled'}")

@app.on_event("shutdown")
async def shutdown_event():
    """Graceful shutdown of all systems"""
    logger.info("üõë Shutting down Royal Bot - Maximum Efficiency Edition")
    
    # Shutdown worker pool first (stop processing new messages)
    logger.info("‚ö° Shutting down worker pool...")
    await shutdown_worker_pool()
    
    # Close context manager connections
    logger.info("üß† Closing context manager connections...")
    if hybrid_context_manager.redis_client:
        await hybrid_context_manager.redis_client.close()
    
    # Close queue connections
    logger.info("üì¨ Closing queue connections...")
    if advanced_queue.redis_client:
        await advanced_queue.redis_client.close()
    
    logger.info("‚úÖ Shutdown complete")

# =====================================================
# MAIN EXECUTION
# =====================================================

if __name__ == "__main__":
    import uvicorn
    
    # Configure uvicorn for Railway
    uvicorn_config = {
        "app": "royal_server_optimized:app",
        "host": "0.0.0.0",
        "port": PORT,
        "log_level": "info",
        "access_log": ENABLE_REQUEST_LOGGING,
        "workers": 1,  # Important: Only 1 worker for proper async coordination
    }
    
    # Production optimizations
    if ENVIRONMENT == "production":
        uvicorn_config.update({
            "reload": False,
            "log_level": "warning",
        })
    else:
        uvicorn_config.update({
            "reload": True,
            "log_level": "debug",
        })
    
    logger.info(f"üöÄ Starting server with config: {uvicorn_config}")
    uvicorn.run(**uvicorn_config)